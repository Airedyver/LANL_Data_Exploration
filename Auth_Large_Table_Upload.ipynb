{"cells":[{"cell_type":"code","source":["#Upload Authorization Table Data, because this data set is LARGE, this will need to be streamed.\n\nimport json\nimport base64\nimport requests\n\nDOMAIN = '<databricks-instance>'\nTOKEN = '<your-token>'\nBASE_URL = 'https://%s/api/2.0/dbfs/' % (DOMAIN)\n\ndef dbfs_rpc(action, body):\n    \"\"\" A helper function to make the DBFS API request, request/response is encoded/decoded as JSON \"\"\"\n        response = requests.post(\n        BASE_URL + action,\n        headers={\"Authorization\": \"Basic \" + base64.standard_b64encode(\"token:\" + TOKEN)},\n        json=body\n    )\n    return response.json()\n\n Create a handle that will be used to add blocks\nhandle = dbfs_rpc(\"create\", {\"path\": \"/temp/upload_large_file\", \"overwrite\": \"true\"})['handle']\nwith open('/a/local/file') as f:\n    while True:\n        #A block can be at most 1MB\n        block = f.read(1 << 20)\n        if not block:\n            break\n        data = base64.standard_b64encode(block)\n        dbfs_rpc(\"add-block\", {\"handle\": handle, \"data\": data})\n close the handle to finish uploading\ndbfs_rpc(\"close\", {\"handle\": handle})"],"metadata":{},"outputs":[],"execution_count":1}],"metadata":{"name":"Auth_Large_Table_Upload","notebookId":1358208303185200},"nbformat":4,"nbformat_minor":0}
